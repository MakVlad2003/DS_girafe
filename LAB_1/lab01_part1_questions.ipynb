{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RoTiyoh_hmB"
   },
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_15IzQCy_hmF"
   },
   "source": [
    "# Lab assignment №1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BzFcseH_hmH"
   },
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEnCjoxo_hmI"
   },
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP4X0Cu7_hmK"
   },
   "source": [
    "Useful links:\n",
    "- [Matrix Differentiation (EN)](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "- [Матричные вычисления (RU)](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDkd6uNG_hmK"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rydKIvN_hmL"
   },
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghu6nYzfAF5G"
   },
   "source": [
    "**Решение:**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{d(x_1^2 + x_2^2 + ... + x_N^2)}{dx} = \\left( \\frac{d(x_1^2)}{dx}, \\frac{d(x_2^2)}{dx}, \\dots, \\frac{d(x_N^2)}{dx} \\right)^T = (2x_1, 2x_2, \\dots, 2x_N)^T = 2x^T.\n",
    "$$\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x^T.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV4WybGs_hmN"
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7evsRA-O_hmO"
   },
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxnzT-Oc_hmP"
   },
   "source": [
    "**Решение:**\n",
    "\n",
    "Заметим, что $i-й$ диагональный элемент матрицы $C = AB$ будут равны скалярному произведению $i-й$ строки матрицы A и $i-го$ столбца матрицы B.\n",
    "\n",
    "Т.е. $i-й$ диагональный элемент представим в виде суммы: $\\sum _{j=1} ^{N} a_{ij} b_{ji}$\n",
    "\n",
    "Тогда представим след матрицы С в виде двойной суммы:\n",
    "\n",
    "$$tr(AB) = \\sum_{i=1}^{N} \\sum_{j=1}^{N}a_{ij}b_{ji}, \\; где$$\n",
    "\n",
    "$a_{ij}$ и $b_{ji}$ это элементы матриц A и B соответственно.\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = \\\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "    \\partial{y}/\\partial{a_{11}} & \\partial{y}/\\partial{a_{12}} & \\cdots & \\partial{y}/\\partial{a_{1N}}\\\\\n",
    "    \\partial{y}/\\partial{a_{21}} & \\partial{y}/\\partial{a_{22}} & \\cdots & \\partial{y}/\\partial{a_{2N}}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\partial{y}/\\partial{a_{N1}} & \\partial{y}/\\partial{a_{N2}} & \\cdots & \\partial{y}/\\partial{a_{NN}}\\\\\n",
    "  \\end{array} } \\right]\n",
    "\\ = \\\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "    b_{11} & b_{21} & \\cdots & b_{N1}\\\\\n",
    "    b_{12} & b_{22} & \\cdots & b_{N2}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    b_{1N} & b_{2N} & \\cdots & b_{NN}\\\\\n",
    "  \\end{array} } \\right]\n",
    "\\ = B^T\n",
    "$$\n",
    "\n",
    "**Ответ:**\n",
    "$$\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-iblI2H_hmP"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_8OcGOd_hmQ"
   },
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoELSr_5_hmQ"
   },
   "source": [
    "**I. Решение:**\n",
    "\n",
    "$$\n",
    "x^TA = \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    x_{1} & x_{2} & \\cdots & x_{N}\\\\\n",
    "  \\end{array} } \\right)\n",
    "\\ * \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1N}\\\\\n",
    "    a_{21} & a_{22} & \\cdots & a_{2N}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    a_{N1} & a_{N2} & \\cdots & a_{NN}\\\\\n",
    "  \\end{array} } \\right)\n",
    "\\ = \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    \\sum _{i=1} ^N x_i a_{i1} & \\sum _{i=1} ^N x_i a_{i2} & \\cdots & \\sum _{i=1} ^N x_i a_{iN}\\\\\n",
    "  \\end{array} } \\right)\n",
    "\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = x^TAc = \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    \\sum _{i=1} ^N x_i a_{i1} & \\sum _{i=1} ^N x_i a_{i2} & \\cdots & \\sum _{i=1} ^N x_i a_{iN}\\\\\n",
    "  \\end{array} } \\right)\n",
    "\\ * \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    c_{1}\\\\\n",
    "    c_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    c_{N}\\\\\n",
    "  \\end{array} } \\right)\n",
    "\\ = \\sum _{j=1} ^N c_j \\sum _{i=1} ^N x_i a_{ij}\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\\n",
    "  \\left( {\\begin{array}{cccc}\n",
    "    \\sum _{j=1} ^N c_j a_{1j} & \\sum _{j=1} ^N c_j a_{2j} & \\dots & \\sum _{j=1} ^N c_j a_{Nj}\n",
    "  \\end{array} } \\right)\n",
    "\\ = c^TA^T\n",
    "$$\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = c^TA^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB_CdSmf_hmQ"
   },
   "source": [
    "**II. Решение:**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = \\\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "    x_1 c_1 & x_1 c_2 & \\cdots & x_1 c_N\\\\\n",
    "    x_2 c_1 & x_2 c_2 & \\cdots & x_2 c_N\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    x_N c_1 & x_N c_2 & \\cdots & x_N c_N\\\\\n",
    "  \\end{array} } \\right]\n",
    "\\ = xc^T, \\; т.к.\n",
    "$$\n",
    "\n",
    "значение $y$ нам известно из предыдущего пункта. Как выглядит в общем виде дифференциал числа по матрице записано в задаче 2.\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = xc^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_P_9dvn_hmQ"
   },
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact\n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZsfMHk6_hmR"
   },
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQTnBJl7_hmR"
   },
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ?\n",
    "$$\n",
    "\n",
    "You may use one of the following approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPA8iwyl_hmR"
   },
   "source": [
    "#### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T)\n",
    "$$\n",
    "it is easy to derive gradients (you can find it in one of the refs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk5MNbWQ_hmS"
   },
   "source": [
    "#### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihwgHav-_hmS"
   },
   "source": [
    "#### Third approach\n",
    "And finally we can use chain rule!\n",
    "let $ F = AS $\n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$\n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_-uz_LON9xY"
   },
   "source": [
    "**Решение:**\n",
    "\n",
    "Воспользуемся тем фактом, что $|| X ||_F^2 = tr(XX^T)$.\n",
    "\n",
    "Тогда для $J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}$:\n",
    "\n",
    "$$J = tr((X - AS)(X - AS)^T) = tr((X - AS)^T(X - AS)) = tr(X^TX) - 2tr(X^TAS) + tr(S^TA^TAS)$$\n",
    "\n",
    "$$ \\frac{\\partial I}{\\partial S} = -2A^TX + 2A^TS = -2A^T(X - AS)$$\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "$$ \\frac{\\partial I}{\\partial S} = -2A^T(X - AS)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol0r_gFd_hmS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions.\n",
    "\n",
    "https://github.com/girafe-ai/ml-course/tree/master/homeworks/assignment0_01_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfxI32r6_hmT"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Если мы наблюдаем полностью светлый ряд, это может указывать на то, что элемент тестовой выборки значительно удален от всех элементов тренировочной выборки. Это может означать, что он является выбросом, принадлежит другому набору данных или просто отличается от остальных элементов в выборке.\n",
    "\n",
    "- What causes the columns?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Полностью светлый столбец, согласно той же логике, указывает на наличие элемента в тренировочной выборке, который существенно удален от всех элементов тестовой выборки. Это может также говорить о том, что данный элемент является выбросом. Для более точного анализа можно создать матрицу расстояний, используя только элементы из тренировочной выборки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7934zqv6_hmT"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,\n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is\n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Ответы и объяснения:\n",
    "\n",
    "1. **Вычитание среднего $\\mu$:**\n",
    "\n",
    "   Вычитание константного среднего из всех точек не изменяет L1 расстояния между ними, так как:\n",
    "   $$\n",
    "   \\forall x_1, x_2: ||(x_1 - \\mu) - (x_2 - \\mu)||_1 = ||x_1 - x_2||_1\n",
    "   $$\n",
    "   Это означает, что расстояния между точками остаются прежними, и следовательно, результат работы классификатора k-ближайших соседей (kNN) останется неизменным. **Этот шаг не повлияет на производительность классификатора.**\n",
    "\n",
    "2. **Вычитание попиксельного среднего $\\mu_{ij}$:**\n",
    "\n",
    "   Аналогично первому случаю, вычитание среднего по каждому пикселю не влияет на L1 расстояние между точками:\n",
    "\n",
    "   $$\n",
    "   \\forall x, y \\in \\mathbb{R}^{nm} : L_1(x, y) = \\sum_{i,j} |x_{ij} - y_{ij}| = \\sum_{i,j} |(x_{ij} - \\mu_{ij}) - (y_{ij} - \\mu_{ij})|\n",
    "   $$\n",
    "\n",
    "   Так как расстояния остаются такими же, результат работы kNN не изменится. **Этот шаг также не повлияет на производительность классификатора.**\n",
    "\n",
    "3. **Вычитание среднего $\\mu$ и деление на стандартное отклонение $\\sigma$:**\n",
    "\n",
    "   Если для всех точек вычесть среднее и разделить на стандартное отклонение, это не изменит их относительные расстояния. Пусть даны три вектора $x$, $y$, $z$, их среднее $\\mu$ и стандартное отклонение $\\sigma$, и пусть $||x - y||_1 < ||x - z||_1$, тогда:\n",
    "\n",
    "   $$\n",
    "   ||\\frac{x - \\mu}{\\sigma} - \\frac{y - \\mu}{\\sigma}||_1 = \\frac{1}{\\sigma}||x -    y||_1 < \\frac{1}{\\sigma}||x - z||_1 = ||\\frac{x - \\mu}{\\sigma} - \\frac{z - \\mu}  {\\sigma}||_1\\\\\n",
    "   \\\\\n",
    "   $$\n",
    "\n",
    "   Поскольку отношение расстояний сохраняется, это преобразование не повлияет на результат работы kNN. **Деление на стандартное отклонение после вычитания среднего не изменит производительность классификатора.**\n",
    "\n",
    "4. **Вычитание попиксельного среднего $\\mu_{ij}$ и деление на попиксельное стандартное отклонение $\\sigma_{ij}$:**\n",
    "\n",
    "   Это преобразование может повлиять на результат, так как оно изменяет относительные расстояния между точками. Рассмотрим пример:\n",
    "   $x = (0, 1), y = (1, 0), z = (1, 2)$, при этом $L_1(x, y) = L_1(x, z) = L_1(y, z) = 2$. Теперь посчитаем попиксельные средние и стандартные отклонения:\n",
    "\n",
    "   $$\\mu_1 = 2/3, \\mu_2 = 1$$\n",
    "   $$\\sigma_1 = \\frac{\\sqrt{6}}{9}, \\sigma_2 = \\frac{\\sqrt{2}}{3}$$\n",
    "\n",
    "   После преобразования получаем новые координаты:\n",
    "   $$x' = (-\\frac{6}{\\sqrt{6}}, 0), y' = (\\frac{3}{\\sqrt{6}}, - \\frac{3}{\\sqrt{2}}), z' = (\\frac{3}{\\sqrt{6}}, \\frac{3}{\\sqrt{2}})$$\n",
    "\n",
    "   Теперь:\n",
    "\n",
    "   $$L_1(x', y') = \\frac{9}{\\sqrt{6}} + \\frac{3}{\\sqrt{2}}, \\quad L_1(y', z') = \\frac{6}{\\sqrt{2}}$$\n",
    "   \n",
    "   Это показывает, что относительные расстояния изменились, хотя изначально они были равны. **Этот шаг может изменить результат работы kNN.**\n",
    "\n",
    "5. **Поворот координатных осей:**\n",
    "\n",
    "   Поворот координатных осей также может изменить результат работы kNN. Рассмотрим пример с точками $x = (0, 1)$, $y = (1, 0)$ и $z = (2, 1)$. Их L1 расстояния равны:\n",
    "   $$L_1(x, y) = L_1(y, z) = 2$$\n",
    "   Если повернуть оси на 45 градусов, используя матрицу поворота:\n",
    "   $$\n",
    "   A = \\left[ \\begin{array}{cc} \\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} \\end{array} \\right]\n",
    "   $$\n",
    "   То новые координаты точек будут:\n",
    "   $$\n",
    "   x' = A \\cdot x = \\left[ \\begin{array}{c} - \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{array} \\right], \\quad y' = A \\cdot y = \\left[ \\begin{array}{c} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{array} \\right], \\quad z' = A \\cdot z = \\left[ \\begin{array}{c} \\frac{3\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{array} \\right]\n",
    "   $$\n",
    "   Теперь:\n",
    "   $$||x' - y'||_1 = 2\\sqrt{2}, \\quad ||y' - z'||_1 = \\frac{5}{2}\\sqrt{2}$$\n",
    "   Таким образом, после поворота равноудалённые точки оказались неравноудалёнными. **Это может повлиять на результат работы kNN.**\n",
    "\n",
    "---\n",
    "\n",
    "**Выводы:** Таким образом, на результат работы kNN с L1 расстоянием не повлияют первые три преобразования, а четвёртое и пятое могут изменить результат работы классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AqZa_9-_hmT"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "### Ответы и объяснения:\n",
    "\n",
    "1. **Ложь**\n",
    "\n",
    "Совсем не обязательно, чтобы границы решений для k-NN классификатора были линейными. Например, если рассмотреть данные типа \"circles\" из библиотеки `sklearn` и применить 3-NN классификатор, границы будут сильно изогнутыми и нелинейными. Это связано с тем, что k-NN классификатор принимает решения на основе ближайших соседей, что позволяет строить границы, адаптированные к сложной структуре данных. Таким образом, в зависимости от расположения точек, границы могут быть очень запутанными и далеко не линейными.\n",
    "\n",
    "2. **Правда**\n",
    "\n",
    "Если взять тренировочный датасет в качестве тестового, то ближайший сосед для точки $ x $ будет сама эта точка $ x $, что даст 100% точность. При выборе 5 ближайших соседей это уже не всегда будет верно, и значение ошибки 0 — это просто нижняя граница для 5-NN. 1-NN всегда идеально классифицирует обучающие данные, тогда как 5-NN может давать ошибки.\n",
    "\n",
    "3. **Ложь**\n",
    "\n",
    "*Рассмотрим пример:*\n",
    "\n",
    "Пусть $X_{\\text{train}} = (-5, -4, -3, -2, -1, 3)$, $y_{\\text{train}} = (0, 0, 0, 0, 0, 1)$.\n",
    "\n",
    "Теперь тестовая точка $x = 2$, и её класс — 0. Для 1-NN классификатора ближайшая точка — это $x = 3$, поэтому ошибка будет равна 1 (ошибочная классификация). Однако для 5-NN классификатора ошибка будет $\\frac{1}{5}$, или даже 0, если округление даёт верный класс в задаче классификации. Таким образом, 5-NN может иметь меньшую ошибку на тестовой выборке, чем 1-NN.\n",
    "\n",
    "4. **Правда**\n",
    "\n",
    "При предсказании мы должны вычислить расстояния между тестовой точкой и каждой тренировочной точкой. Следовательно, с ростом объема тренировочного набора количество вычислений увеличивается, и время классификации тестовой точки возрастает.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
