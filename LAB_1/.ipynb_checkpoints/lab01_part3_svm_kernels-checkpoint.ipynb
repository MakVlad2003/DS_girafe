{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbo9UC4lULHd"
   },
   "source": [
    "# Lab assignment №1, part 3\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tBLyPbkULHe"
   },
   "source": [
    "__*This is the third part of the assignment. First and second parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdInN1OoULHe"
   },
   "source": [
    "##  Part 3. SVM and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7ieS2vKULHf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7b8f71403aa9084",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Kernels concept get adopted in variety of ML algorithms (e.g. Kernel PCA, Gaussian Processes, kNN, ...).\n",
    "\n",
    "So in this task you are to examine kernels for SVM algorithm applied to rather simple artificial datasets.\n",
    "\n",
    "To make it clear: we will work with the classification problem through the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAUdV4dcULHf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f562bf4f554fae",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awRgShwnULHf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b128784928e8df1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's generate our dataset and take a look on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "xH_rLxNEULHf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee8cf8e9cf114b9d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "outputId": "29af2111-232b-4306-f86a-687f21764b5b"
   },
   "outputs": [],
   "source": [
    "moons_points, moons_labels = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "plt.scatter(moons_points[:, 0], moons_points[:, 1], c=moons_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuPoBuC7ULHg",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35b09404d22ab9f4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.1 Pure models.\n",
    "First let's try to solve this case with good old Logistic Regression and simple (linear kernel) SVM classifier.\n",
    "\n",
    "Train LR and SVM classifiers (choose params by hand, no CV or intensive grid search neeeded) and plot their decision regions. Calculate one preffered classification metric.\n",
    "\n",
    "Describe results in one-two sentences.\n",
    "\n",
    "_Tip:_ to plot classifiers decisions you colud use either sklearn examples ([this](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py) or any other) and mess with matplotlib yourself or great [mlxtend](https://github.com/rasbt/mlxtend) package (see their examples for details)\n",
    "\n",
    "_Pro Tip:_ wirte function `plot_decisions` taking a dataset and an estimator and plotting the results cause you want to use it several times below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TwmTDaLULHg",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-550546e70e191bc3",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1VqtfSOVGw_"
   },
   "outputs": [],
   "source": [
    "def plot_decisions(X, y, estimator, ax, title=\"\"):\n",
    "    plot_decision_regions(X=X, y=y, clf=estimator, ax=ax)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43dNNk5SVCOM"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='elasticnet', l1_ratio=0.2, solver='saga', C=0.5)\n",
    "svm = SVC(kernel='linear', C=0.5)\n",
    "\n",
    "lr.fit(moons_points, moons_labels)\n",
    "lr_f1 = f1_score(moons_labels, lr.predict(moons_points))\n",
    "\n",
    "svm.fit(moons_points, moons_labels)\n",
    "svm_f1 = f1_score(moons_labels, svm.predict(moons_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "sAXaWZ71U9PA",
    "outputId": "391f2df0-4c88-463a-8f65-e6d28d9c9c75"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(14, 5), ncols=2)\n",
    "\n",
    "plot_decisions(\n",
    "    X=moons_points, y=moons_labels, estimator=lr, ax=axs[0],\n",
    "    title=f\"Logistic Regression, F1 = {lr_f1:.4f}\"\n",
    ")\n",
    "\n",
    "plot_decisions(\n",
    "    X=moons_points, y=moons_labels, estimator=svm, ax=axs[1],\n",
    "    title=f\"SVM, F1 = {svm_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_59MEJ9ULHg"
   },
   "source": [
    "**Вывод:** видно, что линейные методы без дополнительных улучшений не подходят для решения этой задачи — классы не являются линейно разделимыми. Логистическая регрессия и SVM дают схожие значения F1-метрики (около 0.85), что подчеркивает их ограниченную эффективность в данном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzw0E6a3ULHg"
   },
   "source": [
    "## 1.2 Kernel tirck\n",
    "\n",
    "Now use different kernels (`poly`, `rbf`, `sigmoid`) on SVC to get better results. Play `degree` parameter and others.\n",
    "\n",
    "For each kernel estimate optimal params, plot decision regions, calculate metric you've chosen eariler.\n",
    "\n",
    "Write couple of sentences on:\n",
    "\n",
    "* What have happenned with classification quality?\n",
    "* How did decision border changed for each kernel?\n",
    "* What `degree` have you chosen and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfz7TXmkV7uP"
   },
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'linear': {\n",
    "        'C': [1, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1]\n",
    "    },\n",
    "    'poly': {\n",
    "        'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "        'degree': [2, 3, 4, 5, 6, 7, 8],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "        'coef0': [1, 2, 2.5, 2.8, 2.9, 3, 3.1, 3.2]\n",
    "    },\n",
    "    'rbf': {\n",
    "        'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "        'gamma': ['auto', 'scale']\n",
    "    },\n",
    "    'sigmoid': {\n",
    "        'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "        'coef0': [0.5, 0.75, 0.9, 0.95, 1, 1.05, 1.1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0lvX-UnbV7-S",
    "outputId": "cb893534-a0a0-4815-f574-7563e1d2fd37"
   },
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "fig, axs = plt.subplots(figsize=(12, 12), nrows=2, ncols=2)\n",
    "\n",
    "for i, (kernel, params) in enumerate(param_grids.items()):\n",
    "    svc = SVC(kernel=kernel)\n",
    "    grid = GridSearchCV(svc, params, scoring='f1_macro', n_jobs=-1, cv=5)\n",
    "    grid.fit(moons_points, moons_labels)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    f1 = f1_score(moons_labels, best_model.predict(moons_points))\n",
    "\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    plot_decisions(\n",
    "        X=moons_points, y=moons_labels, estimator=best_model, ax=ax,\n",
    "        title=f\"{kernel.capitalize()} Kernel, F1 = {f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{kernel.capitalize()} Kernel Best Params:\", grid.best_params_)\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9isawHULHh"
   },
   "source": [
    "**Вывод:** полиномиальное и RBF ядра показали хорошие результаты, однако при использовании сигмоиды возникли проблемы с качеством классификации.\n",
    "\n",
    "Тем не менее, RBF не совсем корректно сформировало разделяющую поверхность: на \"половине\" класса 1 с отступом снова появляется \"зона\" класса 0. Полиномиальное ядро такой проблемы не имеет и демонстрирует немного более высокую метрику.\n",
    "\n",
    "Также важно отметить, что для полиномиального ядра критично было выбрать подходящие значения для степени и параметра coef0 — с значением по умолчанию (coef0 = 0) качество модели значительно снижалось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C0inNdMULHh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba9a59e3ec57f514",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.3 Simpler solution (of a kind)\n",
    "What is we could use Logisitc Regression to successfully solve this task?\n",
    "\n",
    "Feature generation is a thing to help here. Different techniques of feature generation are used in real life, couple of them will be covered in additional lectures.\n",
    "\n",
    "In particular case simple `PolynomialFeatures` ([link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)) are able to save the day.\n",
    "\n",
    "Generate the set of new features, train LR on it, plot decision regions, calculate metric.\n",
    "\n",
    "* Comare SVM's results with this solution (quality, borders type)\n",
    "* What degree of PolynomialFeatures have you used? Compare with same SVM kernel parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKGyPNeoULHh"
   },
   "outputs": [],
   "source": [
    "def my_decision_plot(points, labels, clf, figsize, title='', data_transformer=None):\n",
    "    xmin, xmax = points[:, 0].min(), points[:, 0].max()\n",
    "    ymin, ymax = points[:, 1].min(), points[:, 1].max()\n",
    "    x_edges = [xmin - (xmax - xmin) / 10, xmax + (xmax - xmin) / 10]\n",
    "    y_edges = [ymin - (ymax - ymin) / 10, ymax + (ymax - ymin) / 10]\n",
    "\n",
    "    x = np.linspace(*x_edges, 1000)\n",
    "    y = np.linspace(*y_edges, 1000)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    coords = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    if data_transformer:\n",
    "        coords = data_transformer.transform(coords)\n",
    "\n",
    "    pred_grid = clf.predict_proba(coords)[:, 1].reshape(len(x), len(y))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.contourf(xx, yy, pred_grid, levels=50, cmap=\"coolwarm\", alpha=0.7)\n",
    "    plt.colorbar(label=\"Probability of Class 1\")\n",
    "\n",
    "    scatter = plt.scatter(points[:, 0], points[:, 1], c=labels, cmap=\"coolwarm\", edgecolor='k', s=50)\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    plt.title(title)\n",
    "    plt.xlim(x_edges)\n",
    "    plt.ylim(y_edges)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtnLJZvpXB0t"
   },
   "outputs": [],
   "source": [
    "def plot_logistic_poly(degrees, points, labels):\n",
    "    for degree in degrees:\n",
    "        pol_transformer = PolynomialFeatures(degree)\n",
    "        X_pol = pol_transformer.fit_transform(points)\n",
    "\n",
    "        pol_lr = LogisticRegression(penalty='elasticnet', l1_ratio=0.6, solver='saga', max_iter=10000)\n",
    "        pol_lr.fit(X_pol, labels)\n",
    "\n",
    "        pol_lr_f1 = f1_score(labels, pol_lr.predict(X_pol))\n",
    "\n",
    "        my_decision_plot(\n",
    "            points, labels, pol_lr,\n",
    "            figsize=(8, 6),\n",
    "            title=f\"Polynomial Degree = {degree}, F1 = {pol_lr_f1:.4f}\",\n",
    "            data_transformer=pol_transformer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EvidnUcGULHh",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-58a1e03cab2ca349",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    },
    "outputId": "a912638d-5c7c-4136-be24-d82f2c4e2c60"
   },
   "outputs": [],
   "source": [
    "degrees = np.arange(1, 11)\n",
    "plot_logistic_poly(degrees, moons_points, moons_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjCwfEMRULHh"
   },
   "source": [
    "**Вывод:** добавление полиномиальных признаков к логистической регрессии оказалось весьма полезным! Для полиномов степени 3 и 4 метрика достигает уровня SVM с RBF ядром.\n",
    "\n",
    "Однако полиномы более высоких степеней снижают качество: метрика падает, модель начинает давать «размытые» разделяющие поверхности и плохо отражает форму данных. Таким образом, если для полиномиального SVM оптимальными степенями были 6-7, то логистическая регрессия показывает наилучшие результаты с полиномиальными признаками степени 3-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "croXwCdjULHi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-868839a4a8358c59",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.4 Harder problem\n",
    "\n",
    "Let's make this task a bit more challenging via upgrading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "I8L6bNsAULHi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86be614f32559cea",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "outputId": "64577393-c3fe-4a88-8295-76b6f6ae4651"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "circles_points, circles_labels = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_points[:, 0], circles_points[:, 1], c=circles_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfhOQODpYrD-"
   },
   "outputs": [],
   "source": [
    "def evaluate_svm_kernels(X, y):\n",
    "    kernels_params = [\n",
    "        (\"Linear\", SVC(kernel='linear'), {'C': [1, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1]}),\n",
    "        (\"Polynomial\", SVC(kernel='poly'), {\n",
    "            'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "            'degree': [2, 3, 4, 5, 6, 7, 8],\n",
    "            'gamma': ['auto', 'scale'],\n",
    "            'coef0': [1, 2, 2.5, 2.8, 2.9, 3, 3.1, 3.2]\n",
    "        }),\n",
    "        (\"RBF\", SVC(kernel='rbf'), {\n",
    "            'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "            'gamma': ['auto', 'scale']\n",
    "        }),\n",
    "        (\"Sigmoid\", SVC(kernel='sigmoid'), {\n",
    "            'C': [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "            'gamma': ['auto', 'scale'],\n",
    "            'coef0': [0.5, 0.75, 0.9, 0.95, 1, 1.05, 1.1]\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(10, 10), ncols=2, nrows=2)\n",
    "\n",
    "    for idx, (kernel_name, model, param_grid) in enumerate(kernels_params):\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', n_jobs=-1, cv=5)\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        best_params = grid_search.best_params_\n",
    "        f1 = f1_score(y, grid_search.predict(X))\n",
    "\n",
    "        print(f\"{kernel_name} Kernel - Best Params: {best_params}, F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "        plot_decision_regions(X=X, y=y, clf=grid_search, ax=axs[idx // 2, idx % 2])\n",
    "        axs[idx // 2, idx % 2].set_title(f\"{kernel_name} Kernel, F1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "K_Syd-hzYrK5",
    "outputId": "cc57835b-8341-4e79-bfc2-f3292030a3c1"
   },
   "outputs": [],
   "source": [
    "evaluate_svm_kernels(circles_points, circles_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLxFS7nnULHi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7e5a8e0da66afbe",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "And even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "Vn8GnQVEULHi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a98ef8e43822e61",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "outputId": "fa3c60fa-c0d3-43da-e4e4-184069bb57b0"
   },
   "outputs": [],
   "source": [
    "points = np.vstack((circles_points*2.5 + 0.5, moons_points))\n",
    "labels = np.hstack((circles_labels, moons_labels + 2)) # + 2 to distinct moons classes\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(points[:, 0], points[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziHZ90-iULHi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c2a785a2d63ce73",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now do your best using all the approaches above!\n",
    "\n",
    "Tune LR with generated features, SVM with appropriate kernel of your choice. You may add some of your loved models to demonstrate their (and your) strength. Again plot decision regions, calculate metric.\n",
    "\n",
    "Justify the results in a few phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J931fjbULHj",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e61b36ea61909c83",
     "locked": false,
     "points": 40,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "lin = SVC(kernel='linear')\n",
    "lin_grid = {\n",
    "    'C' :  [1, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "  }\n",
    "lin_grid = GridSearchCV(lin, lin_grid, scoring='f1_macro', n_jobs=-1, cv=3)\n",
    "\n",
    "\n",
    "poly = SVC(kernel='poly')\n",
    "poly_grid = {\n",
    "    'C' :  [1, 0.8, 0.6, 0.4, 0.2, 0.1],\n",
    "    'degree' : [4, 5, 6, 7, 8],\n",
    "    'gamma' : ['auto', 'scale'],\n",
    "    'coef0' : [1, 2, 2.5, 2.8, 2.9, 3, 3.2]\n",
    "  }\n",
    "poly_grid = GridSearchCV(poly, poly_grid, scoring='f1_macro', n_jobs=-1, cv=3)\n",
    "\n",
    "\n",
    "rbf = SVC(kernel='rbf')\n",
    "rbf_grid = {\n",
    "    'C' :  [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "    'gamma' : ['auto', 'scale']\n",
    "  }\n",
    "rbf_grid = GridSearchCV(rbf, rbf_grid, scoring='f1_macro', n_jobs=-1, cv=3)\n",
    "\n",
    "\n",
    "sigmoid = SVC(kernel='sigmoid')\n",
    "sigmoid_grid = {\n",
    "    'C' :  [1, 0.9, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1],\n",
    "    'gamma' : ['auto', 'scale'],\n",
    "    'coef0' : [0.5, 0.9, 0.95, 1, 1.1]\n",
    "  }\n",
    "sigmoid_grid = GridSearchCV(sigmoid, sigmoid_grid, scoring='f1_macro', n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nwSy4pcGbEy3",
    "outputId": "406fc495-ef6c-4959-9247-1d580fdb90f9"
   },
   "outputs": [],
   "source": [
    "models = [lin_grid, poly_grid, rbf_grid, sigmoid_grid]\n",
    "\n",
    "names = ['Linear', 'Polynomial', 'rbf', 'sigmoid']\n",
    "fig, axs = plt.subplots(figsize=(10, 10), ncols=2, nrows=2)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        k = 2*i + j\n",
    "        models[k].fit(points, labels)\n",
    "\n",
    "        print(names[k])\n",
    "        print(models[k].best_params_)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        f1 = f1_score(labels, models[k].predict(points), average='macro')\n",
    "        plot_decision_regions(X=points, y=labels, clf=models[k], ax=axs[i, j])\n",
    "        axs[i, j].set_title(f\"{names[k]}, F1 = {f1.round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1vKyHcIULHj"
   },
   "source": [
    "**Вывод:** SVM с полиномиальным ядром действительно продемонстрировал отличные результаты, уверенно справившись с задачей и показав высокие метрики. RBF ядро также оказалось близко по качеству, хотя немного уступает. Линейное ядро и сигмоидное заметно хуже справляются, при этом сигмоида даже слабее линейного. В итоге, полиномиальное ядро с оптимальной степенью 6 оказалось наилучшим решением."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
