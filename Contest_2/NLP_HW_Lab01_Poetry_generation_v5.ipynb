{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GULmtJnqvbvI"
   },
   "source": [
    "# Генерация поэзии с помощью нейронных сетей: шаг 1\n",
    "\n",
    "## Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
    "\n",
    "### Ноутбук выполнил: Макаров Владсилав\n",
    "\n",
    "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Siss6ZZvbvM"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import string\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H945esUdvbvN",
    "outputId": "f408997e-cf08-485b-d0cd-990297d0b34c"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('{} device is available'.format(device))\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPenWOy01Ooa",
    "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
   },
   "source": [
    "#### 1. Загрузка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Xvr9hs6vbvP",
    "outputId": "29606585-ab9d-4107-b7a1-4506a823bbf1"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
    "\n",
    "with open('onegin.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "\n",
    "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQYpmGfR_gJ8"
   },
   "source": [
    "#### 2. Построение словаря и предобработка текста\n",
    "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nqozp99hvbvQ",
    "outputId": "96f23102-b742-419a-e15a-c3e7b61bf018"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "tokens = sorted(set(text.lower())) + ['<sos>']\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "assert num_tokens == 84, \"Check the tokenization process\"\n",
    "\n",
    "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
    "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
    "\n",
    "print(\"Seems fine!\")\n",
    "\n",
    "\n",
    "text_encoded = [token_to_idx[x] for x in text]\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWw50P2KvbvR"
   },
   "source": [
    "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
    "\n",
    "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
    "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
    "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
    "\n",
    "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
    "\n",
    "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2FK4wLgvbvT"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "batch_size = 256\n",
    "seq_length = 100\n",
    "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
    "\n",
    "def generate_chunk():\n",
    "    global text_encoded, start_column, batch_size, seq_length\n",
    "\n",
    "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
    "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
    "    yield np.hstack((start_column, data))\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ZPDNgxvbvT"
   },
   "source": [
    "Пример батча:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxaLZieGvbvU",
    "outputId": "491c4461-3ec8-466e-8397-40f85bb78569"
   },
   "outputs": [],
   "source": [
    "next(generate_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfTJ4qTEvbvV"
   },
   "source": [
    "Далее вам предстоит написать код для обучения модели и генерации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0tzoiGGvbvV"
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size=128, hidden_size=256):\n",
    "        \"\"\"\n",
    "        Простая рекуррентная нейронная сеть (Vanilla RNN)\n",
    "        :param num_tokens: количество уникальных токенов\n",
    "        :param embedding_size: размер эмбеддингов\n",
    "        :param hidden_size: размер скрытого состояния\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_tokens)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Прямой проход через модель.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f04antgSA4wi"
   },
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "seq_length = 100\n",
    "batch_size = 256\n",
    "num_epochs = 10_000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVHv50tAwaCh"
   },
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "model = VanillaRNN(num_tokens, embedding_size, hidden_size).to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUm7-iPD_rXM"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, text_encoded, num_epochs, batch_size, seq_length):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    with tqdm(total=num_epochs, desc=\"Обучение\", unit=\"эпоха\") as pbar:\n",
    "      for epoch in range(num_epochs):\n",
    "          epoch_loss = 0\n",
    "\n",
    "          for _ in range(len(text_encoded) // (batch_size * seq_length)):\n",
    "              # Генерация батча\n",
    "              batch = next(generate_chunk())\n",
    "              x = torch.tensor(batch[:, :-1], dtype=torch.long).to(device)\n",
    "              y = torch.tensor(batch[:, 1:], dtype=torch.long).to(device)\n",
    "\n",
    "              # Прямой проход\n",
    "              logits, _ = model(x)\n",
    "              loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "              # Обратный проход\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "              epoch_loss += loss.item()\n",
    "\n",
    "          avg_loss = epoch_loss / (len(text_encoded) // (batch_size * seq_length))\n",
    "          losses.append(avg_loss)\n",
    "          pbar.set_postfix(loss=f\"{avg_loss:.4f}\")  # Обновляем статус прогресса\n",
    "          pbar.update(1)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN4ZOQ0a_vQu",
    "outputId": "810b293b-1bee-4c3b-dfbf-443ec7b62138"
   },
   "outputs": [],
   "source": [
    "losses = train_model(model, text_encoded, num_epochs, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "7Q7gBxEr_w62",
    "outputId": "171c0e1e-8993-4e37-d95b-bb18ce9e76fc"
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hQml2KkvbvW"
   },
   "source": [
    "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TAwRxxdvbvW",
    "outputId": "81f6e1a0-7de0-41f8-81c6-a62cd41c71d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_N_LSpMvbvW"
   },
   "source": [
    "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrBuiXyzvbvW"
   },
   "outputs": [],
   "source": [
    "def generate_sample(model, seed_phrase=' мой дядя самых честных правил', max_length=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[char] for char in seed_phrase]\n",
    "        x_sequence = torch.tensor([x_sequence], dtype=torch.long).to(device)\n",
    "        hidden = None\n",
    "        generated_text = seed_phrase\n",
    "\n",
    "        for _ in range(max_length - len(seed_phrase)):\n",
    "            logits, hidden = model(x_sequence[:, -1:], hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            next_token = np.random.choice(len(tokens), p=probs.ravel())\n",
    "            next_char = idx_to_token[next_token]\n",
    "\n",
    "            generated_text += next_char\n",
    "            x_sequence = torch.cat([x_sequence, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFQwM9tpvbvX"
   },
   "source": [
    "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtQ6IsXpvbvX",
    "outputId": "4adf40e9-e5c5-4612-af6c-22674af3f245"
   },
   "outputs": [],
   "source": [
    "seed_phrase = ' мой дядя самых честных правил'\n",
    "\n",
    "# Генерация 10 текстов\n",
    "generated_phrases = [\n",
    "    generate_sample(model, seed_phrase=seed_phrase, max_length=500, temperature=0.8)\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "# Проверяем, что тексты сгенерированы\n",
    "print(\"Пример сгенерированного текста:\", generated_phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97BmU6eqAAKh"
   },
   "outputs": [],
   "source": [
    "# Проверяем, что `generated_phrases` содержит 10 текстов длиной 500 символов\n",
    "assert len(generated_phrases) == 10, \"Должно быть 10 текстов!\"\n",
    "assert all(len(phrase) == 500 for phrase in generated_phrases), \"Каждый текст должен быть длиной 500 символов!\"\n",
    "\n",
    "# Проверяем, что токены соответствуют набору символов\n",
    "assert all([x in set(tokens) for phrase in generated_phrases for x in phrase]), \"Неизвестные токены в текстах!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVcwiMeRvbvX"
   },
   "source": [
    "### Сдача задания\n",
    "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
    "\n",
    "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlwcdiqbATwe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yrvXz8nvbvY",
    "outputId": "a7bbe3cd-f5a9-48ed-b693-8db5210db194"
   },
   "outputs": [],
   "source": [
    "# Генерация текстов с помощью обученной модели\n",
    "seed_phrase = ' мой дядя самых честных правил'\n",
    "\n",
    "# Генерируем 10 текстов\n",
    "generated_phrases = [\n",
    "    generate_sample(model, seed_phrase=seed_phrase, max_length=500, temperature=0.8)\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "# Проверяем, что все тексты корректны\n",
    "for phrase in generated_phrases:\n",
    "    assert isinstance(phrase, str), \"Each phrase must be a string\"\n",
    "    assert len(phrase) == 500, \"Each phrase must be exactly 500 characters long\"\n",
    "    assert all([x in tokens for x in phrase]), \"Unknown tokens detected in generated phrases!\"\n",
    "\n",
    "print(\"Все тексты успешно сгенерированы!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TNyfwRQvbvY",
    "outputId": "a39cba7b-340e-4766-8eb4-943b2cd319a1"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "import json\n",
    "if 'generated_phrases' not in locals():\n",
    "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
    "\n",
    "for phrase in generated_phrases:\n",
    "\n",
    "    if not isinstance(phrase, str):\n",
    "        raise ValueError(\"The generated phrase should be a string\")\n",
    "\n",
    "    if len(phrase) != 500:\n",
    "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
    "\n",
    "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
    "\n",
    "\n",
    "submission_dict = {\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'generated_phrases': generated_phrases\n",
    "}\n",
    "\n",
    "with open('submission_dict.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gImIE4_ivbvY"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
